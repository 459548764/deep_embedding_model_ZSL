{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np, h5py\n",
    "import scipy.io as sio\n",
    "import sys\n",
    "import random\n",
    "import kNN_cosine\n",
    "import re\n",
    "from numpy import *   \n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def compute_accuracy(test_word, test_visual, test_id, test_label):\n",
    "    global left_w1     \n",
    "    word_pre = sess.run(left_w1, feed_dict={word_features: test_word})\n",
    "    test_id = np.squeeze(np.asarray(test_id))\n",
    "    outpre = [0]*6180  \n",
    "    test_label = np.squeeze(np.asarray(test_label))\n",
    "    test_label = test_label.astype(\"float32\")\n",
    "    for i in range(6180):  \n",
    "        outputLabel = kNN_cosine.kNNClassify(test_visual[i,:], word_pre, test_id, 1) \n",
    "        outpre[i] = outputLabel\n",
    "    correct_prediction = tf.equal(outpre, test_label)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={\n",
    "                                           word_features: test_word, visual_features: test_visual})\n",
    "    return result\n",
    "\n",
    "\n",
    "# # data\n",
    "\n",
    "f=sio.loadmat('./data/AwA_data/wordvector/train_word.mat')\n",
    "word=np.array(f['train_word'])\n",
    "word.shape\n",
    "\n",
    "f=sio.loadmat('./data/AwA_data/train_googlenet_bn.mat')\n",
    "x=np.array(f['train_googlenet_bn'])\n",
    "x.shape\n",
    "\n",
    "f=sio.loadmat('./data/AwA_data/test_googlenet_bn.mat')\n",
    "x_test=np.array(f['test_googlenet_bn'])\n",
    "x_test.shape\n",
    "\n",
    "f=sio.loadmat('./data/AwA_data/test_labels.mat')\n",
    "test_label=np.array(f['test_labels'])\n",
    "test_label.shape\n",
    "\n",
    "f=sio.loadmat('./data/AwA_data/testclasses_id.mat')\n",
    "test_id=np.array(f['testclasses_id'])\n",
    "test_id.shape\n",
    "\n",
    "f=sio.loadmat('./data/AwA_data/wordvector/test_vectors.mat')\n",
    "word_pro=np.array(f['test_vectors'])\n",
    "word_pro.shape\n",
    "\n",
    "\n",
    "\n",
    "# # data shuffle\n",
    "def data_iterator():\n",
    "    \"\"\" A simple data iterator \"\"\"\n",
    "    batch_idx = 0\n",
    "    while True:\n",
    "        # shuffle labels and features\n",
    "        idxs = np.arange(0, len(x))\n",
    "        np.random.shuffle(idxs)\n",
    "        shuf_visual = x[idxs]\n",
    "        shuf_word = word[idxs]\n",
    "        batch_size = 64\n",
    "        for batch_idx in range(0, len(x), batch_size):\n",
    "            visual_batch = shuf_visual[batch_idx:batch_idx+batch_size]\n",
    "            visual_batch = visual_batch.astype(\"float32\")\n",
    "            word_batch = shuf_word[batch_idx:batch_idx+batch_size]\n",
    "            yield word_batch, visual_batch \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# # Placeholder\n",
    "# define placeholder for inputs to network\n",
    "word_features = tf.placeholder(tf.float32, [None, 1000])\n",
    "visual_features = tf.placeholder(tf.float32, [None, 1024])\n",
    "\n",
    "\n",
    "# # Network\n",
    "# AwA 1000 1024 ReLu, 1e-3 * regularisers, 64 batch, 0.0001 Adam\n",
    "W_left_w1 = weight_variable([1000, 1024])\n",
    "b_left_w1 = bias_variable([1024])\n",
    "left_w1 = tf.nn.relu(tf.matmul(word_features, W_left_w1) + b_left_w1)\n",
    "\n",
    "\n",
    "# # loss\n",
    "loss_w = tf.reduce_mean(tf.square(left_w1 - visual_features))\n",
    "\n",
    "# L2 regularisation for the fully connected parameters.             \n",
    "regularisers_w = (tf.nn.l2_loss(W_left_w1) + tf.nn.l2_loss(b_left_w1))\n",
    "                  \n",
    "                  \n",
    "# Add the regularisation term to the loss.\n",
    "loss_w += 1e-3 * regularisers_w\n",
    "\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_w)\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# # Run\n",
    "iter_ = data_iterator()\n",
    "for i in range(1000000):\n",
    "    word_batch_val, visual_batch_val = iter_.next()\n",
    "    sess.run(train_step, feed_dict={word_features: word_batch_val, visual_features: visual_batch_val})\n",
    "    if i % 1000 == 0:\n",
    "        print(compute_accuracy(word_pro, x_test, test_id, test_label))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
